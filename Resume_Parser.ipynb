{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HassanRiaz1992/Resume-Parser-in-Spacy-3/blob/main/Resume_Parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "S6u7X-u5L6mY"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Js9z2fjpay4u",
        "outputId": "ab1429e0-03a6-4830-d189-a4dd8fad66d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy_transformers\n",
            "  Downloading spacy_transformers-1.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.8/190.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<4.0.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from spacy_transformers) (3.5.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy_transformers) (1.22.4)\n",
            "Collecting transformers<4.31.0,>=3.4.0 (from spacy_transformers)\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from spacy_transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from spacy_transformers) (2.4.6)\n",
            "Collecting spacy-alignments<1.0.0,>=0.7.2 (from spacy_transformers)\n",
            "  Downloading spacy_alignments-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (1.1.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (4.65.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy_transformers) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy_transformers) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy_transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy_transformers) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->spacy_transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->spacy_transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->spacy_transformers) (16.0.5)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers<4.31.0,>=3.4.0->spacy_transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<4.31.0,>=3.4.0->spacy_transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<4.31.0,>=3.4.0->spacy_transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<4.31.0,>=3.4.0->spacy_transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<4.31.0,>=3.4.0->spacy_transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers<4.31.0,>=3.4.0->spacy_transformers) (2023.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.5.0->spacy_transformers) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.5.0->spacy_transformers) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4.0.0,>=3.5.0->spacy_transformers) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->spacy_transformers) (1.3.0)\n",
            "Installing collected packages: tokenizers, safetensors, spacy-alignments, huggingface-hub, transformers, spacy_transformers\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 spacy-alignments-0.9.0 spacy_transformers-1.2.5 tokenizers-0.13.3 transformers-4.30.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.2)\n",
            "Installing collected packages: spacy\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.5.2\n",
            "    Uninstalling spacy-3.5.2:\n",
            "      Successfully uninstalled spacy-3.5.2\n",
            "Successfully installed spacy-3.5.3\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy_transformers\n",
        "!pip install -U spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3M4WPhZc-DL",
        "outputId": "53ba4a52-bafc-4b1f-b001-5a200b8c5cd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCOTfHutdOt9",
        "outputId": "13afa52e-646b-4227-fbae-e92c18716b2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyresparser\n",
            "  Downloading pyresparser-1.0.6-py3-none-any.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (23.1.0)\n",
            "Requirement already satisfied: blis>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (0.7.9)\n",
            "Requirement already satisfied: certifi>=2019.6.16 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (2022.12.7)\n",
            "Requirement already satisfied: chardet>=3.0.4 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (4.0.0)\n",
            "Requirement already satisfied: cymem>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (2.0.7)\n",
            "Collecting docx2txt>=0.7 (from pyresparser)\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (3.4)\n",
            "Requirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (4.3.3)\n",
            "Requirement already satisfied: nltk>=3.4.3 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (1.22.4)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (1.5.3)\n",
            "Collecting pdfminer.six>=20181108 (from pyresparser)\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m112.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: preshed>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (3.0.8)\n",
            "Collecting pycryptodome>=3.8.2 (from pyresparser)\n",
            "  Downloading pycryptodome-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyrsistent>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (0.19.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2019.1 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (2022.7.1)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (2.27.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (1.16.0)\n",
            "Requirement already satisfied: sortedcontainers>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (2.4.0)\n",
            "Requirement already satisfied: spacy>=2.1.4 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (3.5.3)\n",
            "Requirement already satisfied: srsly>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (2.4.6)\n",
            "Requirement already satisfied: thinc>=7.0.4 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (8.1.9)\n",
            "Requirement already satisfied: tqdm>=4.32.2 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (4.65.0)\n",
            "Requirement already satisfied: urllib3>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (1.26.15)\n",
            "Requirement already satisfied: wasabi>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from pyresparser) (1.1.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4.3->pyresparser) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4.3->pyresparser) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4.3->pyresparser) (2022.10.31)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six>=20181108->pyresparser) (2.0.12)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six>=20181108->pyresparser) (40.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from preshed>=2.0.1->pyresparser) (1.0.9)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.4->pyresparser) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.4->pyresparser) (1.0.4)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.4->pyresparser) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.4->pyresparser) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.4->pyresparser) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.4->pyresparser) (6.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.4->pyresparser) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.4->pyresparser) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.4->pyresparser) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.4->pyresparser) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.1.4->pyresparser) (3.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc>=7.0.4->pyresparser) (0.0.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six>=20181108->pyresparser) (1.15.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy>=2.1.4->pyresparser) (4.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=2.1.4->pyresparser) (2.1.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six>=20181108->pyresparser) (2.21)\n",
            "Building wheels for collected packages: docx2txt\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3957 sha256=095f2a487612018bdc45cefc75fd48b9cf3a7527c430ceb0b614d99a2c0b2b65\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/58/cf/093d0a6c3ecfdfc5f6ddd5524043b88e59a9a199cb02352966\n",
            "Successfully built docx2txt\n",
            "Installing collected packages: docx2txt, pycryptodome, pdfminer.six, pyresparser\n",
            "Successfully installed docx2txt-0.8 pdfminer.six-20221105 pycryptodome-3.18.0 pyresparser-1.0.6\n"
          ]
        }
      ],
      "source": [
        "!pip install pyresparser"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "rgrrpUkMHI-Y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cv_data = pd.read_pickle(r'/content/sample_data/Resume/train_data.pkl')"
      ],
      "metadata": {
        "id": "-R8J9wqnHQVG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_data = json.load(open('/content/train_data.json'))"
      ],
      "metadata": {
        "id": "vaQLX91jmpGr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(cv_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjCNdp6dIEwz",
        "outputId": "da7a7f99-c9e8-4bc5-bac6-49b2839e31c9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy init fill-config /content/base_config.cfg config.cfg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2bD-7VhJv_P",
        "outputId": "78ddbf4b-7ac4-40bc-8136-e816d150a1ed"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-15 17:32:49.641821: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv_data[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0aGSIYJMPU0",
        "outputId": "277cf828-41fa-4246-c84e-b6bcbb12fced"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Ijas Nizamuddin Associate Consultant - State Street  Irinchayam B.O, Kerala - Email me on Indeed: indeed.com/r/Ijas- Nizamuddin/6748d77f76f94eed  With close to 3 years of experience in IT industry, I have had excellent exposure to design, development and implementation of Client Server Applications in various domains such as Banking and Finance concepts. I have been involved in various software Development projects in Open System environment.  WORK EXPERIENCE  Associate Consultant  Oracle Corporation -  June 2011 to Present  State Street Global Advisors (SSgA) is the asset management business of State Street Corporation, one of the world's leading providers of financial services to institutional investors1, with a heritage dating back over two centuries. Backed by the strength and stability of the State Street organization, SSgA makes continual investments in asset management and client service platform, resulting in a client-focused, solutions-driven orientation .BrokerViews is the application which list all the details about the counterparties who invest their securities in State Street.The details also include ratings given by Bloomberg.  Responsibilities: Development, Testing and support. Software Used: Java, GWT  Associate Consultant  Oracle Corporation -  Bangalore, Karnataka -  May 2010 to June 2011  This project is actually a redesign of an existing client website. The client website was designed on Java Server Pages (JSP) and our aim was to change it into a more dynamic web page using Adobe Flex. At first we changed the home page screen of the client website. After the successful completion of that we incorporated flex in to the account section also. This data which is obtained from DataBase is taken by the flex using a remote procedure call and the data is shown to the user. With the use of Advanced Data Grids, Charts(including Bar and Pie Charts) the site increased the readability and understandability of the users who were previously using the pages on java server pages. This site developed by us won the IMC (Interactive Media Council)'s outstanding achievement award in Financial information. The judge evaluate website based on 5 criteria: Design, Content, Feature Functionality, Usability and Standard Compliance. Our website scored 475 out of a maximum of 500 points. . Responsibilities: Development, Testing and support. Software Used: Java, Adobe Flex  https://www.indeed.com/r/Ijas-Nizamuddin/6748d77f76f94eed?isid=rex-download&ikw=download-top&co=IN https://www.indeed.com/r/Ijas-Nizamuddin/6748d77f76f94eed?isid=rex-download&ikw=download-top&co=IN   Framework: Springs, MVC  Associate Consultant  Oracle Corporation -  Bangalore, Karnataka -  February 2010 to April 2010  Description: Development of Basel II Application, Basel II is the second of the Basel Accords, which are recommendations on banking laws and regulations issued by the Basel Committee on Banking Supervision. The purpose of Basel II, which was initially published in June 2004, is to create an international standard that banking regulators can use when creating regulations about how much capital banks need to put aside to guard against the types of financial and operational risks banks face. In practice, Basel II attempts to accomplish this by setting up rigorous risk and capital management requirements designed to ensure that a bank holds capital reserves appropriate to the risk the bank exposes itself to through its lending and investment practices.  The New Accord includes several methodologies for determining a bank's risk-based capital requirements for credit, market and operational risk. For Risk Based Capital (RBC), Credit Usage (CU) and Stress Test (ST), the methodologies that will be used for repo style transactions are Simple VaR if the collateral is eligible. If the collateral is ineligible then the Wholesale loan approach will be utilized or the collateral will be reduced to zero in the Simple VaR. . Roles and Responsibilities: Development, Testing and support. Software Used: Oracle 9i  OTHER PROJECTS AND REAL TIME TRAINING:  RTRM(Railway Ticketing System Through Mobile) A mobile based real time application with many exciting features like checking pnr status, train availability, trains between stations etc. This application was done in j2me and it uses weblogic as server and MSSQL as the database.  Undergone a mandatory Training on Finance By Oracle Corporation  EDUCATION  Birla Institute Of Technology -  Pilani, Rajasthan  2011  Bachelor of Technology in Computer Science  University College Of Engineering, University Of Kerala  2005 to 2009  ADDITIONAL INFORMATION  SKILL SET:    Languages: Core Java Front end/GUI Tools programming: Adobe Flex, GWT Database: Oracle 10g IDE: Eclipse, FlexBuilder FrameWorks: Spring(Basics), MVC frame work Operating System: Windows, Linux, Unix\",\n",
              " {'entities': [[4652, 4850, 'Skills'],\n",
              "   [4608, 4612, 'Graduation Year'],\n",
              "   [4543, 4576, 'College Name'],\n",
              "   [4499, 4541, 'Degree'],\n",
              "   [4493, 4497, 'Graduation Year'],\n",
              "   [4441, 4470, 'College Name'],\n",
              "   [4410, 4428, 'Companies worked at'],\n",
              "   [2654, 2672, 'Companies worked at'],\n",
              "   [2632, 2652, 'Designation'],\n",
              "   [1323, 1327, 'Graduation Year'],\n",
              "   [1260, 1278, 'Companies worked at'],\n",
              "   [1238, 1258, 'Designation'],\n",
              "   [603, 615, 'Companies worked at'],\n",
              "   [487, 505, 'Companies worked at'],\n",
              "   [465, 485, 'Designation'],\n",
              "   [98, 144, 'Email Address'],\n",
              "   [53, 67, 'Location'],\n",
              "   [39, 51, 'Companies worked at'],\n",
              "   [16, 36, 'Designation'],\n",
              "   [0, 15, 'Name']]}]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_spacy_doc(file, data):\n",
        "    nlp = spacy.blank('en') #created an empty pipeline for english language\n",
        "    db = DocBin() # created a doc bin object\n",
        "      # iterate the dataset which we want to convert into spacy format\n",
        "    for text, annot in tqdm(data):\n",
        "        # convert the dataset of resume into text format\n",
        "        doc = nlp.make_doc(text)\n",
        "        # identify the annotation from data\n",
        "        annot = annot['entities']\n",
        "        # store the entities into an array\n",
        "        ents = []\n",
        "        # Number of characteristics\n",
        "        entity_indices = []\n",
        "\n",
        "        # Extracting information of entities from the data set in iteration\n",
        "        #start, end and label are like 16, 36, 'Designation'\n",
        "        for start, end, label in annot:\n",
        "            # No Entity shall be skip from extraction\n",
        "            skip_entity = False\n",
        "            # if an entity character is already present in the entity_indices than we shall skip the entity and and cotinure the process\n",
        "            for idx in range(start, end):\n",
        "                if idx in entity_indices:\n",
        "                    skip_entity = True\n",
        "                    break\n",
        "            if skip_entity == True:\n",
        "                continue\n",
        "            # entity indices are passed to the array of entity_indices and list of star and end maintained\n",
        "            entity_indices = entity_indices + list(range(start, end))\n",
        "             # we need to check that either annotated data is producing error or not\n",
        "            try:\n",
        "                span = doc.char_span(start, end,label=label,alignment_mode = 'strict')\n",
        "            except:\n",
        "                continue\n",
        "              # if span has nothing than we need tocheck that at span has error\n",
        "            if span is None:\n",
        "                err_data = str([start, end]) + \" \" + str(text) + \"\\n \"\n",
        "                file.write(err_data)\n",
        "            else:\n",
        "                ents.append(span)\n",
        "        # putting all the extracted entities in the in doc which later added to docbin\n",
        "        try:\n",
        "            doc.ents = ents\n",
        "            db.add(doc)\n",
        "        except:\n",
        "            pass\n",
        "    return db"
      ],
      "metadata": {
        "id": "vV_erestNLvN"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(cv_data, test_size=0.1)"
      ],
      "metadata": {
        "id": "roF2ez2dXVCt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " len(train),len(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uec9ybHaXvvu",
        "outputId": "3704345d-348b-4324-af39-6a17ec05b23f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(180, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('error.txt', 'w', encoding='utf-8')\n",
        "\n",
        "db = get_spacy_doc(file, train)\n",
        "db.to_disk('train_data.spacy')\n",
        "\n",
        "db = get_spacy_doc(file, test)\n",
        "db.to_disk('test_data.spacy')\n",
        "\n",
        "file.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oySlm81rYDmS",
        "outputId": "728443e8-b72c-4f9a-e4b8-3d1ce88ff72d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 180/180 [00:03<00:00, 56.39it/s]\n",
            "100%|██████████| 20/20 [00:00<00:00, 44.77it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "len(db.tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6kXSsP-ZrFN",
        "outputId": "8ff7df34-b7a2-463c-8567-a0702ec5bfb3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import subprocess\n",
        "# Spacy command\n",
        "#config_path = \"/content/config.cfg\"\n",
        "#train_data_path = \"./content/train_data.spacy\"\n",
        "#test_data_path = \"./content/test_data.spacy\"\n",
        "#output_path = \"./output\"\n",
        "\n",
        "#command = [\"python\", \"-m\", \"spacy\", \"train\", config_path, \"--output\", output_path, \"--paths.train\", train_data_path, \"--paths.dev\", test_data_path]\n",
        "#subprocess.run(command)"
      ],
      "metadata": {
        "id": "jOpfmZF0bskg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train /content/config.cfg --output ./output --paths.train ./train_data.spacy --paths.dev ./test_data.spacy --gpu-id 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYaXXsPIYKpw",
        "outputId": "3cae33e0-decb-4d64-dd66-924f915b134a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-15 17:34:03.943974: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;2m✔ Created output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2023-06-15 17:34:16,472] [INFO] Set up nlp object from config\n",
            "[2023-06-15 17:34:16,488] [INFO] Pipeline: ['transformer', 'ner']\n",
            "[2023-06-15 17:34:16,491] [INFO] Created vocabulary\n",
            "[2023-06-15 17:34:16,491] [INFO] Finished initializing nlp object\n",
            "Downloading (…)lve/main/config.json: 100% 481/481 [00:00<00:00, 3.14MB/s]\n",
            "Downloading (…)olve/main/vocab.json: 100% 899k/899k [00:00<00:00, 10.5MB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 7.21MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 44.6MB/s]\n",
            "Downloading model.safetensors: 100% 499M/499M [00:08<00:00, 55.6MB/s]\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[2023-06-15 17:34:51,948] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "  0       0        4729.74   1574.44    0.47    0.24   10.89    0.00\n",
            "  3     200      114281.66  62813.25   29.93   38.41   24.51    0.30\n",
            "  6     400       44469.48  23020.88   51.08   51.59   50.58    0.51\n",
            "  9     600       11697.86  20062.64   55.28   57.87   52.92    0.55\n",
            " 12     800        4335.53  17929.74   57.09   56.98   57.20    0.57\n",
            " 15    1000        3457.16  16592.97   55.51   54.28   56.81    0.56\n",
            " 18    1200       12361.64  15870.09   54.65   53.33   56.03    0.55\n",
            " 21    1400        1507.52  14668.02   58.85   60.16   57.59    0.59\n",
            " 24    1600       14737.71  14483.27   54.05   53.64   54.47    0.54\n",
            " 27    1800        2103.98  13589.18   56.69   58.20   55.25    0.57\n",
            " 30    2000         744.20  12854.54   54.81   57.76   52.14    0.55\n",
            " 33    2200        1207.18  12320.25   53.10   59.05   48.25    0.53\n",
            " 36    2400        1505.41  11754.31   54.59   62.19   48.64    0.55\n",
            " 39    2600         616.74  10975.08   55.58   60.55   51.36    0.56\n",
            " 42    2800        1908.19  10301.28   55.84   67.78   47.47    0.56\n",
            " 45    3000        2208.32   9495.16   52.99   58.77   48.25    0.53\n",
            "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
            "output/model-last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NLP = spacy.load('/content/output/model-best')"
      ],
      "metadata": {
        "id": "yJiWvbiApoLD"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def pdf_to_text(pdf_path):\n",
        "    text = \"\"\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        num_pages = len(pdf_reader.pages)\n",
        "\n",
        "        for page in range(num_pages):\n",
        "            pdf_page = pdf_reader.pages[page]\n",
        "            text += pdf_page.extract_text()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Provide the path to your PDF file\n",
        "pdf_path = '/content/Smith_Resume.pdf'\n",
        "\n",
        "# Convert PDF to text\n",
        "cv_text = pdf_to_text(pdf_path)\n",
        "\n",
        "# Print the extracted text\n",
        "print(cv_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_DGSDMeVPvy",
        "outputId": "16111595-42e8-427e-859a-ccb309d76fd4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Michael Smith \n",
            "BI / Big Data/ Azure  \n",
            "Manchester , UK- Email me on Indeed:  indeed.com/r/ falicent/140749dace5dc26f  \n",
            " \n",
            "10+ years of Experience in Designing, Development, Administration, Analysis, \n",
            "Management inthe Business Intelligence Da ta warehousing, Client Server \n",
            "Technologies, Web -based Applications, cloud solutions and Databases.  \n",
            "Data warehouse: Data analysis, star/ snow flake schema data modeling and design \n",
            "specific todata warehousing and business intelligence environment.  \n",
            "Database: Experience in database designing, scalability, back -up and recovery, \n",
            "writing andoptimizing SQL code and Stored Procedures, creating functions, views, \n",
            "triggers and indexes.  \n",
            "Cloud platform: Worked on Microsoft Azure cloud services like Document DB, SQL \n",
            "Azure, StreamAnalytics, Event hub, Power BI, Web Job, Web App, Power BI, Azure \n",
            "data lake analytics(U -SQL). \n",
            "Big Data: Worked Azure data lake store/analytics for big data processing and Azure \n",
            "data factoryto schedule U -SQL jobs. Designed and developed end to end big data \n",
            "solution for data insights.  \n",
            " \n",
            "Willing to relocate: Anywhere  \n",
            "WORK EXPERIENCE Software Engineer  \n",
            "Microsoft - Manchester , UK. \n",
            "December 2015 to Pre sent \n",
            "1. Microsoft Rewards Live dashboards:  \n",
            "Description: - Microsoft rewards is loyalty program that rewards Users for \n",
            "browsing and shopping online. Microsoft Rewards members can earn points when \n",
            "searching with Bing, browsing with Microsoft Edge and making purchases at the \n",
            "Xbox Store, the Windows St ore and the Microsoft Store. Plus, user can pick up \n",
            "bonus points for taking daily quizzes and tours on the Microsoft rewards website. \n",
            "Rewards live dashboards gives a live picture of usage world -wide and by markets \n",
            "like US, Canada, Australia, new user regis tration count, top/bottom performing \n",
            "rewards offers, orders stats and weekly trends of user activities, orders and new \n",
            "user registrations. the PBI tiles gets refreshed in different frequencies starting \n",
            "from 5 seconds to 30 minutes.  \n",
            "Technology/Tools used  \n",
            "Event hub, stream analytics and Power BI.  \n",
            "Responsibilities  \n",
            "Created stream analytics jobs to process event hub data  \n",
            "Created Power BI live dashboard to show live usage traffic, weekly trends, cards, \n",
            "charts to showtop/bottom 10 offers and usage metrics.  \n",
            "2. Microsoft Rewards Data Insights:  \n",
            "Description: - Microsoft rewards is loyalty program that rewards Users for \n",
            "browsing and shopping online. Microsoft Rewards members can earn points when \n",
            "searching with Bing, browsing with Microsoft Edge and making purchases at t he \n",
            "Xbox Store, the Windows Store and the Microsoft Store. Plus, user can pick up \n",
            "bonus points for taking daily quizzes and tours on the Microsoft rewards website. \n",
            "Rewards data insights is data analytics and reporting platform, processes 20 \n",
            "million users da ily activities and redemption across different markets like US, \n",
            "Canada, Australia.  \n",
            "Technology/Tools used  \n",
            "Cosmos (Microsoft big -data platform), c#, X -flow job monitoring, Power BI.  \n",
            "Responsibilities  Created big data scripts in cosmos  \n",
            "C# data extractors, proc essors and reducers for data transformation  \n",
            "Power BI dashboards  \n",
            "3. End to end tracking Tool:  \n",
            "Description: - This is real -time Tracking tool to track different business \n",
            "transactions like order, order response, functional acknowledgement, invoice \n",
            "flowing ins ide ICOE. It gives flexibility to customers to track their transactions \n",
            "and appropriate error information in -case of any failure. Based on resource based \n",
            "access control the tool gives flexibility to end user to perform different actions \n",
            "like view transacti ons, search based on different filter criteria and view and \n",
            "download actual message payload. End to end tracking tool stitches all the \n",
            "business transaction like order to cash flow and connects different hops inside \n",
            "ICOE like gateway, routing server, Proces sing server. It also connects different \n",
            "systems like ICOE, partner end point and SAP.  \n",
            "Technology/Tools used  \n",
            "Azure Document db, Azure web job and Web APP, RBAC, Angular JS.  \n",
            "Responsibilities  \n",
            "Document dB stored procedures.  \n",
            "Web job to process event hub data and populate Document db• Web App API.  \n",
            "Stream analytics job to transform data  \n",
            "Power BI reports  \n",
            "4. Biztrack Tracking Tool:  \n",
            "Description: - This is real -time Tracking tool to track different business \n",
            "transactions like order, order response, functional acknowledgement, invoice \n",
            "flowing inside ICOE. It gives flexibility to customers to track their transactions \n",
            "and appropriate error  information in -case of any failure. Based on resource based \n",
            "access control the tool gives flexibility to end user to perform different actions \n",
            "like view transactions, search based on different filter criteria and view and \n",
            "download actual message payload.  \n",
            "Technology/Tools used  \n",
            "SQL server 2014, SSIS, .net API, Angular JS.  \n",
            "Responsibilities  \n",
            "ETL solution to transform business transactions data stored in Biztalk tables.  \n",
            "SQL azure tables, stored procedures, User defined functions.  \n",
            "Performance tuning.  \n",
            "Web API enha ncements.  \n",
            " \n",
            "EDUCATION  \n",
            "The University of Manchester  - UK \n",
            "2007 \n",
            " \n",
            "SKILLS \n",
            "problem solving (Less than 1 year), project lifecycle (Less than 1 year), project \n",
            "manager (Less than 1 year), technical assistance. (Less than 1 year)  \n",
            "ADDITIONAL INFORMATION  \n",
            "Professiona l Skills  \n",
            "Excellent analytical, problem solving, communication, knowledge transfer and \n",
            "interpersonalskills with ability to interact with individuals at all the levels  \n",
            "Quick learner and maintains cordial relationship with project manager and team \n",
            "members and good performer both in team and independent job environments  \n",
            "Positive attitude towards superiors &amp; peers  Supervised junior developers throughout project lifecycle and provided technical \n",
            "assistance.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = NLP(cv_text)\n",
        "for ent in doc.ents:\n",
        "    print(ent.label_.upper(),\"  =>  \", ent.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZKGqhUoWqdn",
        "outputId": "94c1e90a-e229-4672-aac1-de96bf1c01c2"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME   =>   Michael Smith\n",
            "LOCATION   =>   Manchester\n",
            "EMAIL ADDRESS   =>   indeed.com/r/ falicent/140749dace5dc26f\n",
            "COMPANIES WORKED AT   =>   Microsoft\n",
            "DESIGNATION   =>   Software Engineer\n",
            "COMPANIES WORKED AT   =>   Microsoft\n",
            "COMPANIES WORKED AT   =>   Microsoft\n",
            "COMPANIES WORKED AT   =>   Microsoft\n",
            "COMPANIES WORKED AT   =>   Microsoft\n",
            "COMPANIES WORKED AT   =>   Microsoft\n",
            "COMPANIES WORKED AT   =>   Microsoft\n",
            "COMPANIES WORKED AT   =>   Microsoft\n",
            "COMPANIES WORKED AT   =>   Microsoft\n",
            "COMPANIES WORKED AT   =>   Microsoft\n",
            "COMPANIES WORKED AT   =>   Microsoft\n",
            "COMPANIES WORKED AT   =>   Microsoft\n",
            "COMPANIES WORKED AT   =>   Microsoft\n",
            "COMPANIES WORKED AT   =>   Microsoft\n",
            "COMPANIES WORKED AT   =>   Microsoft\n",
            "COLLEGE NAME   =>   The University of Manchester\n",
            "GRADUATION YEAR   =>   2007\n",
            "SKILLS   =>   problem solving (Less than 1 year), project lifecycle (Less than 1 year), project \n",
            "manager (Less than 1 year), technical assistance. (Less than 1 year)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert PDF to text\n",
        "cv_text = pdf_to_text('/content/Resume Hassan (1).pdf')\n",
        "\n",
        "# Print the extracted text\n",
        "print(cv_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FvfI7xGXBaH",
        "outputId": "43941d3d-f18a-49c2-cc8f-d928a3d0f599"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Hassan Riaz \n",
            "Contact  \n",
            "Address:  \n",
            "Flat 07 , Block C-6, ZTBL Colony , G-\n",
            "7/2, Islamabad  \n",
            " \n",
            "Phone:  \n",
            "+92 0331 5547995   \n",
            " \n",
            "Email:  \n",
            "tipus816@gmail.com  \n",
            " \n",
            "Languages  \n",
            "Urdu  – A1 \n",
            "English  – A2 \n",
            "Hobbies  \n",
            " Reading  \n",
            " Writing  \n",
            " \n",
            "Repository  \n",
            "https://github.com/HassanRiaz19\n",
            "92 Summary  \n",
            "Seeking a job in challenging and healthy working environment \n",
            "where I cherish challenges and aspire to seek excellence in the task \n",
            "assigned to me on the basis of my knowledge and experience . \n",
            "Skill Highlights  \n",
            " Mult i tasking  \n",
            " Strong decision maker  \n",
            " Complex problem \n",
            "solver   Creative  \n",
            " Innovative  \n",
            " \n",
            "Experience  \n",
            "Officer Grade II  - 07/201 7 to Till date  \n",
            "Zarai Taraqiati Bank Limited , Islamabad  \n",
            " Preparation evaluation and submission of Sales Tax \n",
            "Return along with reconciliation of Other Indirect Taxes . \n",
            " Evaluation for medical expense and it budgeting.  \n",
            " \n",
            "Statistical Assistant  - 02/201 7 to 07/2017  \n",
            "Pakistan Bureau of Statisitcs , Islamabad  \n",
            " \n",
            " Managing Census Operation of FATA.  \n",
            "  Providing Logistic Support for Field Officers.  \n",
            " Census Data Cleaning  \n",
            " \n",
            "Education  \n",
            "Masters  of Science: Computer Science  (2023) : Virtual \n",
            "University , Islamabad  \n",
            "Masters of Science: Economics  (2015) : Preston University , \n",
            "Islamabad  \n",
            "Certifications  \n",
            "Introduction to Machine Learning:  Kaggle  \n",
            "Intermediate Machine Learning:  Kaggle . \n",
            "Introduction to Deep Learning:  Kaggle  \n",
            " \n",
            "Project s \n",
            " Online Payments Fraud Detection system using Artificial \n",
            "Intelligence . \n",
            "  \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = NLP(cv_text)\n",
        "for ent in doc.ents:\n",
        "    print(ent.label_.upper(),\"  =>  \", ent.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9H1PBjoY0QJ",
        "outputId": "99908306-db71-447c-c3fa-dc657de8ba49"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME   =>   Hassan Riaz\n",
            "DEGREE   =>   Masters  of Science: Computer Science\n",
            "GRADUATION YEAR   =>   2023\n",
            "DEGREE   =>   Masters of Science: Economics\n",
            "GRADUATION YEAR   =>   2015\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM7xRHEw5wz85k8tz1fHk5m",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}